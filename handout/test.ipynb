{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "from transformers import CLIPTokenizer\n",
    "import torch\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = (torch.bfloat16 if torch_device == \"cuda\" else torch.float32)\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CLIPTokenizer.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"tokenizer\", torch_dtype=torch_dtype,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"hippopot\"\n",
    "y = \"incomprehensible\"\n",
    "\n",
    "words_x = x.split(' ')\n",
    "words_y = y.split(' ')\n",
    "\n",
    "max_len = tokenizer.model_max_length\n",
    "mapper = np.zeros((max_len, max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_inds(text: str, word_place: int, tokenizer):\n",
    "    \"\"\"\n",
    "    Splits the text into words. If 'word_place' is a string, it finds all occurrences of the word in the text and stores their indices. \n",
    "    If 'word_place' is an integer, it wraps it in a list for consistent processing. \n",
    "    Encodes the text into tokens and decodes each token back into string form to identify the boundaries of each word in the tokenized version. \n",
    "    It iterates over these tokens, matching them to the specified word indices ('word_place') and collecting the corresponding token indices in the output list 'out'.\n",
    "    \"\"\"\n",
    "    split_text = text.split(\" \")\n",
    "    if type(word_place) is str:\n",
    "        word_place = [i for i, word in enumerate(split_text) if word_place == word]\n",
    "    elif type(word_place) is int:\n",
    "        word_place = [word_place]\n",
    "    out = []\n",
    "    if len(word_place) > 0:\n",
    "        words_encode = [tokenizer.decode([item]).strip(\"#\") for item in tokenizer.encode(text)][1:-1]\n",
    "        cur_len, ptr = 0, 0\n",
    "\n",
    "        for i in range(len(words_encode)):\n",
    "            cur_len += len(words_encode[i])\n",
    "            if ptr in word_place:\n",
    "                out.append(i + 1)\n",
    "            if cur_len >= len(split_text[ptr]):\n",
    "                ptr += 1\n",
    "                cur_len = 0\n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(words_y)): \n",
    "    if words_y[i] != words_x[i]:\n",
    "        diff_indices = i \n",
    "        \n",
    "source_ind = get_word_inds(x, diff_indices, tokenizer)\n",
    "target_ind = get_word_inds(y, diff_indices, tokenizer)\n",
    "\n",
    "i = j = 0  \n",
    "\n",
    "while i < len(mapper) and j < len(mapper):\n",
    "    if i == source_ind[0]:\n",
    "        if len(source_ind) == len(target_ind):\n",
    "            for s_idx, t_idx in zip(source_ind, target_ind):\n",
    "                mapper[s_idx, t_idx] = 1.0\n",
    "        elif len(source_ind) > len(target_ind):\n",
    "            ratio = 1.0 / len(source_ind)\n",
    "            for t_idx in target_ind:\n",
    "                for s_idx in source_ind:\n",
    "                    mapper[s_idx, t_idx] = ratio\n",
    "        else:\n",
    "            ratio = 1.0 / len(target_ind)\n",
    "            for s_idx in source_ind:\n",
    "                for t_idx in target_ind:\n",
    "                    mapper[s_idx, t_idx] = ratio\n",
    "        i += len(source_ind)\n",
    "        j += len(target_ind)\n",
    "    else:\n",
    "            mapper[i, j] = 1.0\n",
    "            i += 1\n",
    "            j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.33333333, 0.33333333, 0.33333333, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.33333333, 0.33333333, 0.33333333, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapper[:6, :6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# differing_indices = [i for i, (wx, wy) in enumerate(zip(words_x, words_y)) if wx != wy]\n",
    "# move_r, move_c = 0, 0\n",
    "# idx = 0\n",
    "# mapper[0][0] = 1\n",
    "# while (idx + move_r + 1) < len(mapper) and (idx + move_c + 1) < len(mapper):\n",
    "#     if idx in differing_indices:\n",
    "#             tokens_x = tokenizer(words_x[idx], add_special_tokens=False, return_tensors=\"np\")['input_ids'][0]\n",
    "#             tokens_y = tokenizer(words_y[idx], add_special_tokens=False, return_tensors=\"np\")['input_ids'][0]\n",
    "#             len_x, len_y = len(tokens_x), len(tokens_y)\n",
    "#             move_c += (len_y-1)\n",
    "#             move_r += (len_x-1)\n",
    "#             if (len_y * len_x) == len_y or (len_y * len_x) == len_x:\n",
    "#                 val = 1 / (len_y * len_x)\n",
    "#             else:\n",
    "#                 val = 1 / len_y \n",
    "#             for i in range(len_x):\n",
    "#                 for j in range(len_y):\n",
    "#                         mapper[idx + i + 1][idx + j + 1] = val\n",
    "                    \n",
    "#     else:\n",
    "#         mapper[idx + move_r + 1][idx + move_c + 1] = 1\n",
    "#     idx += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapper1 = mapper.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "\n",
    "def get_word_inds(text: str, word_place: int, tokenizer):\n",
    "    \"\"\"\n",
    "    Splits the text into words. If 'word_place' is a string, it finds all occurrences of the word in the text and stores their indices. \n",
    "    If 'word_place' is an integer, it wraps it in a list for consistent processing. \n",
    "    Encodes the text into tokens and decodes each token back into string form to identify the boundaries of each word in the tokenized version. \n",
    "    It iterates over these tokens, matching them to the specified word indices ('word_place') and collecting the corresponding token indices in the output list 'out'.\n",
    "    \"\"\"\n",
    "    split_text = text.split(\" \")\n",
    "    if type(word_place) is str:\n",
    "        word_place = [i for i, word in enumerate(split_text) if word_place == word]\n",
    "    elif type(word_place) is int:\n",
    "        word_place = [word_place]\n",
    "    out = []\n",
    "    if len(word_place) > 0:\n",
    "        words_encode = [tokenizer.decode([item]).strip(\"#\") for item in tokenizer.encode(text)][1:-1]\n",
    "        cur_len, ptr = 0, 0\n",
    "\n",
    "        for i in range(len(words_encode)):\n",
    "            cur_len += len(words_encode[i])\n",
    "            if ptr in word_place:\n",
    "                out.append(i + 1)\n",
    "            if cur_len >= len(split_text[ptr]):\n",
    "                ptr += 1\n",
    "                cur_len = 0\n",
    "    return np.array(out)\n",
    "\n",
    "mapper = np.zeros((max_len, max_len))\n",
    "diff_indices = [i for i in range(len(words_y)) if words_y[i] != words_x[i]]\n",
    "source_inds = [get_word_inds(x, i, tokenizer) for i in diff_indices]\n",
    "target_inds = [get_word_inds(y, i, tokenizer) for i in diff_indices]\n",
    "i = 0\n",
    "j = 0\n",
    "current = 0\n",
    "while i < max_len and j < max_len:\n",
    "    \n",
    "    if current < len(source_inds) and source_inds[current][0] == i:\n",
    "        source_inds_1 = source_inds[current]\n",
    "        target_inds_1 = target_inds[current]\n",
    "        \n",
    "        \n",
    "        if len(source_inds_1) == len(target_inds_1):\n",
    "            \n",
    "            for src_idx, tgt_idx in zip(source_inds_1, target_inds_1):\n",
    "                mapper[src_idx, tgt_idx] = 1.0\n",
    "        elif len(source_inds_1) > len(target_inds_1):\n",
    "            \n",
    "            ratio = 1.0 / len(source_inds_1)\n",
    "            for tgt_idx in target_inds_1:\n",
    "                for src_idx in source_inds_1:\n",
    "                    mapper[src_idx, tgt_idx] = ratio\n",
    "        else:\n",
    "            \n",
    "            ratio = 1.0 / len(target_inds_1)\n",
    "            for src_idx in source_inds_1:\n",
    "                for tgt_idx in target_inds_1:\n",
    "                    mapper[src_idx, tgt_idx] = ratio\n",
    "        \n",
    "\n",
    "        current += 1\n",
    "        i += len(source_inds_1)\n",
    "        j += len(target_inds_1)\n",
    "    else:\n",
    "        \n",
    "        mapper[i, j] = 1.0\n",
    "        i += 1\n",
    "        j += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert mapper1.all() == mapper.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [49406, 585, 739, 512, 12050, 14507, 49407], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it was incomprehensible'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.33333333, 0.33333333,\n",
       "        0.33333333],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapper[0:6, :6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py31",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
